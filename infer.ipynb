{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Здесь содержится функция-пример осуществления предикта любо из 5 моделей, участвующих в предскзаании"
      ],
      "metadata": {
        "id": "KWogJruRGyuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "88BZrJhXGxdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    BertForSequenceClassification,\n",
        ")"
      ],
      "metadata": {
        "id": "QfiJcSzFi_8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, tokenizer, text_to_predict):\n",
        "    print('predict here')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text_to_predict,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        return_token_type_ids=False,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    out = {\n",
        "        'text': text_to_predict,\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten()\n",
        "    }\n",
        "\n",
        "    input_ids = out[\"input_ids\"].to(device)\n",
        "    attention_mask = out[\"attention_mask\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "        input_ids=input_ids.unsqueeze(0),\n",
        "        attention_mask=attention_mask.unsqueeze(0)\n",
        "    )\n",
        "\n",
        "    prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "    return prediction  # not cat, numerical"
      ],
      "metadata": {
        "id": "PDmJcqfxirXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst_a = ['A', 'AA', 'AAA', 'B', 'BB', 'BBB', 'C'] # label decoding for small"
      ],
      "metadata": {
        "id": "477u31lMirZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst = ['A', 'A+', 'A-', 'AA', 'AA+', 'AA-', 'AAA', 'B', 'B+', 'B-', 'BB', 'BB+', 'BB-', 'BBB', 'BBB+', 'BBB-', 'C'] # label decoding for large"
      ],
      "metadata": {
        "id": "KMnUh7M2ircH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}